### 背景：机器学习

考虑到许多使用ML-Agents工具包的用户可能没有正式的机器学习背景，本页面提供了一个概述，以便理解ML-Agents工具包。然而，我们不会尝试提供全面的机器学习处理，因为在线有许多优秀的资源。

机器学习是人工智能的一个分支，重点是从数据中学习模式。机器学习算法主要有三类：无监督学习、监督学习和强化学习。每类算法从不同类型的数据中学习。以下段落概述了每种机器学习的类别，并提供了入门示例。

#### 无监督学习

无监督学习的目标是对数据集中的相似项目进行分组或聚类。例如，考虑游戏的玩家。我们可能希望根据他们对游戏的投入程度对玩家进行分组。这使我们能够针对不同组进行操作（例如，对于高度投入的玩家，我们可能邀请他们成为新功能的测试者，而对于不投入的玩家，我们可能通过电子邮件发送有帮助的教程）。假设我们希望将玩家分成两组。我们首先定义玩家的基本属性，例如游戏时间、在应用内购买上花费的总金额和完成的关卡数量。然后，我们可以将这些数据集（每个玩家的三个属性）输入到一个无监督学习算法中，并指定将玩家分成两组。该算法将根据这些属性将所有玩家分成两组，其中每组中的玩家彼此相似。给定我们用来描述每个玩家的属性，输出结果将是将所有玩家分成两组，其中一组语义上表示投入的玩家，另一组语义上表示不投入的玩家。

在无监督学习中，我们没有提供具体的示例说明哪些玩家被认为是投入的，哪些不是。我们只是定义了适当的属性，并依靠算法自行发现这两组。这种数据集通常称为未标记数据集，因为它缺乏这些直接标签。因此，无监督学习在这些标签昂贵或难以生成的情况下非常有用。在下一段中，我们概述了接受输入标签和属性的监督学习算法。

#### 监督学习

在监督学习中，我们不仅要将相似项目分组，还要直接学习每个项目到其所属组（或类别）的映射。回到我们早先对玩家进行聚类的示例，我们现在希望预测哪些玩家将流失（即在接下来的30天内停止玩游戏）。我们可以查看历史记录，并创建一个包含玩家属性以及是否流失的标签的数据集。注意，我们用于流失预测任务的玩家属性可能与我们早先用于聚类任务的属性不同。然后，我们可以将这些数据集（每个玩家的属性和标签）输入到一个监督学习算法中，该算法将学习从玩家属性到表示该玩家是否流失的标签的映射。直观地说，监督学习算法将学习这些属性的哪些值通常对应于流失和不流失的玩家（例如，它可能会学习那些花费很少且玩耍时间很短的玩家最有可能流失）。现在，给定这个学习到的模型，我们可以提供一个新玩家的属性（一个最近开始玩游戏的玩家），它将输出该玩家的预测标签。这一预测是算法对该玩家是否会流失的预期。我们现在可以使用这些预测来针对预期流失的玩家，诱使他们继续玩游戏。

正如您可能已经注意到的那样，对于无监督学习和监督学习，有两个任务需要执行：属性选择和模型选择。属性选择（也称为特征选择）涉及选择我们希望如何表示感兴趣的实体（在本例中为玩家）。模型选择涉及选择执行任务效果良好的算法（及其参数）。这两个任务都是机器学习研究的活跃领域，在实践中，需要多次迭代以达到良好的性能。

我们现在转向第三类机器学习算法，强化学习，这可能是与ML-Agents工具包最相关的。

#### 强化学习

强化学习可以被看作是一种用于序列决策的学习形式，通常与控制机器人相关（但实际上要广泛得多）。考虑一个自主灭火机器人，任务是进入某个区域，找到火源并将其扑灭。在任何给定时刻，机器人通过其传感器（如摄像头、热感应器、触感器）感知环境，处理这些信息并产生一个动作（如向左移动、旋转水管、打开水管）。换句话说，它不断地根据其对环境的观察（即传感器输入）和目标（即扑灭火源）来做出与环境互动的决策。教一个机器人成为一个成功的灭火机器，正是强化学习的目标。

更具体地说，强化学习的目标是学习一个策略，该策略本质上是一个从观察到动作的映射。观察是机器人从其环境中测量到的内容（在本例中是其所有的传感输入），而动作在最原始的形式下是对机器人配置的改变（例如，其底座的位置、水管的位置以及水管的开关状态）。

强化学习任务的最后一个组成部分是奖励信号。机器人被训练来学习一个策略，该策略最大化其总体奖励。当训练机器人成为一个高效的灭火机器时，我们会根据其完成任务的情况给它奖励（正面和负面）。注意，机器人在训练之前并不知道如何灭火。它学习这个目标是因为当它扑灭火源时会得到一个很大的正面奖励，而每过一秒都会得到一个小的负面奖励。奖励是稀疏的（即可能不会在每一步都提供，而只有在机器人达到成功或失败的情境时才提供），这是强化学习的一个定义特征，这也是为什么在复杂环境中学习好的策略会很困难（和/或耗时）。

强化学习生命周期通常需要多次试验和迭代策略更新。更具体地说，机器人被放置在多个火灾情境中，并随着时间的推移学习到一个最佳策略，使其更有效地扑灭火源。显然，我们不能指望在现实世界中反复训练一个机器人，特别是涉及火灾的情况下。这正是为什么使用Unity作为模拟器是学习这些行为的理想训练场。虽然我们的讨论集中在机器人上，但机器人和游戏中的角色之间有很强的相似性。实际上，在很多方面，可以将一个非玩家角色（NPC）视为一个虚拟机器人，具有自己的环境观察、自己的动作集和特定的目标。因此，探索如何使用强化学习在Unity中训练行为是很自然的。这正是ML-Agents工具包所提供的。下面的视频包括一个展示使用ML-Agents工具包训练角色行为的强化学习演示。

与无监督学习和监督学习相似，强化学习也涉及两个任务：属性选择和模型选择。属性选择是定义一组有助于机器人完成目标的观察，而模型选择是定义策略的形式（从观察到动作的映射）及其参数。在实践中，训练行为是一个迭代过程，可能需要更改属性和模型的选择。

#### 训练和推理

所有三种机器学习类别的共同点是都涉及训练阶段和推理阶段。虽然训练和推理阶段的细节对于每个类别是不同的，但从高层次来看，训练阶段涉及使用提供的数据构建模型，而推理阶段涉及将该模型应用于新的、以前未见过的数据。更具体地说：

- 对于我们的无监督学习示例，训练阶段根据描述现有玩家的数据学习最优的两个聚类，而推理阶段将新玩家分配到这两个聚类之一。
- 对于我们的监督学习示例，训练阶段学习从玩家属性到玩家标签（是否流失）的映射，推理阶段根据该学习到的映射预测新玩家是否会流失。
- 对于我们的强化学习示例，训练阶段通过引导试验学习最优策略，而在推理阶段，代理根据其学习到的策略进行观察和采取行动。

简而言之，所有三类算法都涉及训练和推理阶段，以及属性和模型选择。最终区分它们的是可用于学习的数据类型。在无监督学习中，我们的数据集是一个属性集合；在监督学习中，我们的数据集是一个属性-标签对的集合；最后，在强化学习中，我们的数据集是一个观察-动作-奖励三元组的集合。

#### 深度学习

深度学习是一组可以用于解决上述任何问题的算法。更具体地说，它们可以用来解决属性选择和模型选择任务。近年来，由于其在若干具有挑战性的机器学习任务中的出色表现，深度学习获得了广泛关注。一个例子是AlphaGo，这个利用深度学习的计算机围棋程序，能够击败围棋世界冠军李世石。

深度学习算法的一个关键特征是它们能够从大量训练数据中学习非常复杂的函数。这使得它们在大量数据可以生成的情况下成为强化学习任务的自然选择，例如通过使用像Unity这样的模拟器或引擎。通过在Unity中生成成千上万的环境模拟，我们可以学习非常复杂环境的策略（复杂环境是指代理感知的观察数量和可以采取的动作数量都很大）。我们在ML-Agents中提供的许多算法都使用某种形式的深度学习

，基于开源库PyTorch构建。

### 参考链接
- [无监督学习](https://en.wikipedia.org/wiki/Unsupervised_learning)
- [监督学习](https://en.wikipedia.org/wiki/Supervised_learning)
- [强化学习](https://en.wikipedia.org/wiki/Reinforcement_learning)
- [Unity AI: 强化学习与Q学习](https://blogs.unity3d.com/2017/08/22/unity-ai-reinforcement-learning-with-q-learning/)
- [通过模拟设计更安全的城市](https://blogs.unity3d.com/2018/01/23/designing-safer-cities-through-simulations/)
- [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo)
- [计算机围棋](https://en.wikipedia.org/wiki/Computer_Go)